{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb1aa87-d0cd-4c51-9778-20584dd5d3a7",
   "metadata": {},
   "source": [
    "### 原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c07115d-d44c-4525-8300-780fe771461c",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/1895815215712547750\n",
    "- http(sse) vs. stdio\n",
    "    -  stdio: 本地标准io\n",
    "        -   npm 或 npx 启动\n",
    "    -  http(sse)：remote\n",
    "        -  服务器发送事件 (Server-Sent Events, SSE)：SSE 是一种基于 HTTP 的协议，允许服务器通过一个持久的 HTTP 连接向客户端单向推送数据\n",
    "        -  sse：本地 http 部署\n",
    "-  llm\n",
    "    -  `tools` vs. system prompt\n",
    "        -  Cherry Studio 实际上是通过将 MCP Server 中提供的工具、响应结果，转换为 Function Call 的标准格式来和模型进行交互。\n",
    "        -  Cline 将 MCP Server 中提供的工具、响应结果转换未一套自己约定的输入、输出的标准数据格式，通过系统提示词来声明这种约定，再和模型进行交互。\n",
    "        -  这也解释了，为什么在 Cherry Studio 中只有一部分模型支持 MCP，前提是选择的模型需要支持 Function Call 的调用，并且在客户端进行了特殊适配；而 Cline 则使用的是系统提示词，所以所有模型都支持。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1670627c-7676-4330-a9bf-075dea9b6fec",
   "metadata": {},
   "source": [
    "### 拆解 sequentialthinking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b132d44-12e6-4dd6-ae75-ecaede4ac842",
   "metadata": {},
   "source": [
    "> 规划一个5天的在北京的旅行计划，用 sequencialthinking 工具 \n",
    "\n",
    "- https://github.com/modelcontextprotocol/servers/tree/main/src/sequentialthinking\n",
    "- listTools, callToll\n",
    "  \n",
    "```ts\n",
    "server.setRequestHandler(ListToolsRequestSchema, async () => ({\n",
    "  tools: [SEQUENTIAL_THINKING_TOOL],\n",
    "}));\n",
    "\n",
    "server.setRequestHandler(CallToolRequestSchema, async (request) => {\n",
    "  if (request.params.name === \"sequentialthinking\") {\n",
    "    return thinkingServer.processThought(request.params.arguments);\n",
    "  }\n",
    "\n",
    "  return {\n",
    "    content: [{\n",
    "      type: \"text\",\n",
    "      text: `Unknown tool: ${request.params.name}`\n",
    "    }],\n",
    "    isError: true\n",
    "  };\n",
    "});\n",
    "```\n",
    "- thoughtHistory 的维护\n",
    "    - server 内部会维护 history，但向 mcp host 只返回 length，mcp host 中的 llm 自己在上下文中维护 history\n",
    "    - server：它的职责是成为一个有状态的工具。它就像一个有记忆的计算器，你不断给它输入，它在内部更新自己的状态。它只需要记住“思考”的上下文，并根据这个上下文处理新的“思考”。\n",
    "    - Host/Agent 的职责: 它的职责是决策。它根据 Server 返回的 nextThoughtNeeded标志来决定是否需要继续调用工具，进行下一步思考。它不需要知道历史记录的每一个细节，因为它（或者说它管理的 LLM）本身就是这些历史的创造者\n",
    "- 为什么需要这个工具？LLM 本身不就可以进行“链式思考” (Chain of Thought) 吗？\n",
    "    - 这是一个非常核心的问题。答案在于，这个工具为 LLM的思考过程提供了**外部的、结构化的、可交互的“脚手架”**，解决了原生“链式思考”的一些固有弊端。\n",
    "    - 原生链式思考 (CoT) 的局限\n",
    "        - 当你在提示 (Prompt) 中要求一个 LLM \"一步一步地思考\"时，它会在一次单一的、连续的生成中完成所有思考和回答。这个过程是：\n",
    "            - * 整体的 (Monolithic): 思考和最终答案是一起生成的，无法将思考的每一步拆分成独立单元。\n",
    "            - * 不可中断的 (Uninterruptible): 你无法在 LLM思考到第三步时暂停它，检查一下，或者给它一点新的信息，让它修正第三步然后继续。\n",
    "            - * 难以观察和调试 (Poor Observability): 整个思考过程混在一大段文本里，对于开发者来说，很难以编程方式去解析和验证每一步的正确性。\n",
    "            - * 修正成本高 (Costly to Correct): 如果 LLM 在第 2 步犯了错，但直到第 10 步才意识到，它通常需要重新生成整个思考链，或者在结尾附加一大段修正说明，这会使上下文变得混乱\n",
    "    - sequencialthinking\n",
    "        - 将思考过程“检查点化” (Checkpointing the Process)\n",
    "        - 实现真正的交互与修正 (Enabling True Interaction and Correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b26f85-92db-44be-8d73-d397f41950c5",
   "metadata": {},
   "source": [
    "### uv 与 npx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c3f25a-8b92-4f4c-9a33-f5504c9ab36e",
   "metadata": {},
   "source": [
    "> 均非全局安装？\n",
    "\n",
    "- uv 是 python 的运行环境\n",
    "    -  `uvx` == `uv run tool`\n",
    "-  npx: node js 的运行环境\n",
    "    -  `npx excalidraw-mcp`\n",
    "    -  `npm install -g excalidraw-mcp`\n",
    "        -  globally install"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
